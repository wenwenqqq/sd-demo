import torch
import os
from collections import OrderedDict
from diffusion_model import UNet_Transformer, NoiseScheduler, sample_cfg, sample
from stable_diffusion_model import StableDiffusion
from transformers import CLIPTokenizer, CLIPTextModel
from peft import PeftModel, LoraConfig
from PIL import Image
import numpy as np

# ===================== è¶…å‚æ•° =====================
device = "cuda" if torch.cuda.is_available() else "cpu"
image_size = 64          # latent_size
vae_image_size = 512     # VAE è¾“å…¥/è¾“å‡ºåƒç´ å°ºå¯¸
in_channels = 4          # UNet è¾“å…¥ latent é€šé“æ•°

# âœ… ä¿®å¤ 1ï¼šå‡å°‘æ¨ç†æ—¶é—´æ­¥ï¼ˆ1000â†’100ï¼Œé€Ÿåº¦æå‡ 10 å€ï¼‰
num_timesteps = 1000     # è®­ç»ƒæ—¶çš„æ—¶é—´æ­¥ï¼ˆç”¨äº noise_schedulerï¼‰
inference_steps = 100    # âœ… æ¨ç†æ—¶çš„å®é™…æ­¥æ•°ï¼ˆå¯è°ƒæ•´ 50-200ï¼‰

# ===================== LoRA é…ç½® =====================
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["query", "key", "value", "out_proj"],
    bias="none",
    task_type="CUSTOM",
)

# ===================== è·¯å¾„é…ç½® =====================
sd_model_path = "/data/bdd100k/sd/stable_diffusion_model_final.pth"
lora_ckpt_dir = "/data/bdd100k/sd/stable_diffusion_results/lora_weights_epoch_5"
clip_local_path = "/data/bdd100k/sd/models--openai--clip-vit-base-patch32/snapshots/clip-vit-base-patch32/"

# ===================== åˆå§‹åŒ–æ¨¡å‹ =====================
print("ğŸ”„ åˆå§‹åŒ– StableDiffusion æ¨¡å‹...")
model = StableDiffusion(
    in_channels=3,
    latent_dim=4,
    image_size=vae_image_size,
    diffusion_timesteps=num_timesteps,
    device=device
)

# ===================== åŠ è½½æƒé‡ =====================
print(f"ğŸ”„ åŠ è½½ SD æ¨¡å‹æƒé‡ï¼š{sd_model_path}")
checkpoint = torch.load(sd_model_path, map_location=device, weights_only=True)
state_dict = checkpoint.get('model_state_dict', checkpoint)

unet_state_dict = OrderedDict()
for k, v in state_dict.items():
    k_clean = k.replace('module.', '') if k.startswith('module.') else k
    if k_clean.startswith('unet.'):
        unet_state_dict[k_clean.replace('unet.', '')] = v

model.unet.load_state_dict(unet_state_dict, strict=False)
print("âœ… UNet åŸºç¡€æƒé‡åŠ è½½å®Œæˆ")

vae_state_dict = {k.replace('module.', '').replace('vae.', ''): v 
                  for k, v in state_dict.items() 
                  if k.replace('module.', '').startswith('vae.')}
if vae_state_dict:
    model.vae.load_state_dict(vae_state_dict)
    print("âœ… VAE æƒé‡åŠ è½½å®Œæˆ")

model.to(device)
model.eval()

# ===================== åŠ è½½ LoRA =====================
print(f"ğŸ”„ åŠ è½½ LoRA æƒé‡ï¼š{lora_ckpt_dir}")
model.unet = PeftModel.from_pretrained(
    model.unet, 
    lora_ckpt_dir,
    config=lora_config,
    is_trainable=False
)
print("âœ… LoRA æƒé‡åŠ è½½å®Œæˆ")

# ===================== åŠ è½½ CLIP =====================
print("ğŸ”„ åŠ è½½ CLIP æ–‡æœ¬ç¼–ç å™¨...")
tokenizer = CLIPTokenizer.from_pretrained(clip_local_path)
text_encoder = CLIPTextModel.from_pretrained(clip_local_path).to(device)
text_encoder.eval()
print("âœ… CLIP åŠ è½½å®Œæˆ")

# ===================== æ¨ç†é…ç½® =====================
os.makedirs("inference_output", exist_ok=True)

prompts = [
    "a green bird with a red tail and a black nose",
    "a cute pokemon with blue fur and yellow cheeks",
]
guidance_scale = 7.5

# ===================== æ–‡æœ¬ç¼–ç  =====================
print("ğŸ”„ ç¼–ç æ–‡æœ¬æ¡ä»¶...")
all_embeddings = []
for prompt in prompts:
    text_input = tokenizer(
        [prompt], 
        padding="max_length", 
        max_length=tokenizer.model_max_length, 
        truncation=True, 
        return_tensors="pt"
    )
    with torch.no_grad():
        embedding = text_encoder(text_input.input_ids.to(device)).last_hidden_state
    all_embeddings.append(embedding)
print("âœ… æ–‡æœ¬ç¼–ç å®Œæˆ")

# ===================== å›¾åƒç”Ÿæˆï¼ˆCFG é‡‡æ ·ï¼‰ =====================
print(f"ğŸ¨ å¼€å§‹ç”Ÿæˆå›¾åƒï¼ˆCFG é‡‡æ ·ï¼Œ{inference_steps}æ­¥ï¼‰...")

with torch.no_grad():
    for i, text_emb in enumerate(all_embeddings):
        text_emb = text_emb.to(device)
        
        latent = sample_cfg(
            model.unet,
            model.noise_scheduler,
            n_samples=1,
            in_channels=4,
            text_embeddings=text_emb,
            image_size=image_size,
            guidance_scale=guidance_scale
        )
        
        # VAE è§£ç 
        image = model.vae.decode(latent)
        
        # âœ… ä¿®å¤ 2ï¼šå…ˆå»é™¤ batch ç»´åº¦å† permute
        image = image.squeeze(0).detach().cpu().permute(1, 2, 0).numpy()
        image = (np.clip(image, 0, 1) * 255).astype(np.uint8)
        
        save_path = f"inference_output/generated_cfg_{i}.png"
        Image.fromarray(image).save(save_path)
        print(f"  âœ“ å·²ä¿å­˜ï¼š{save_path}")

# ===================== æ™®é€šé‡‡æ ·ï¼ˆåŠ é€Ÿç‰ˆï¼‰ =====================
print(f"ğŸ¨ å¼€å§‹ç”Ÿæˆå›¾åƒï¼ˆæ™®é€šé‡‡æ ·ï¼Œ{inference_steps}æ­¥ï¼‰...")

with torch.no_grad():
    for i, text_emb in enumerate(all_embeddings):
        text_emb = text_emb.to(device)
        x_t = torch.randn(1, 4, image_size, image_size).to(device)
        
        # è·³æ­¥é‡‡æ ·åŠ é€Ÿ
        skip = max(1, num_timesteps // inference_steps)
        timesteps = list(reversed(range(0, num_timesteps, skip)))[:inference_steps]
        
        for t in timesteps:
            # âœ… å…³é”®ä¿®å¤ï¼šç”¨ [t] åˆ›å»º 1-d tensorï¼Œè€Œä¸æ˜¯ t
            t_tensor = torch.tensor([t], device=device, dtype=torch.long)  # â† æ³¨æ„æ–¹æ‹¬å·
            
            x_t = sample(
                model.unet, 
                x_t, 
                model.noise_scheduler, 
                t_tensor,  # â† ä¼  Tensor è€Œé int
                text_emb
            )
        
        # VAE è§£ç 
        image = model.vae.decode(x_t)
        
        # å»é™¤ batch ç»´åº¦
        image = image.squeeze(0).detach().cpu().permute(1, 2, 0).numpy()
        image = (np.clip(image, 0, 1) * 255).astype(np.uint8)
        
        save_path = f"inference_output/generated_normal_{i}.png"
        Image.fromarray(image).save(save_path)
        print(f"  âœ“ å·²ä¿å­˜ï¼š{save_path}")