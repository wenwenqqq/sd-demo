
===== Epoch 0 训练前 阶段 GPU 显存信息 (rank=0) =====
GPU 0 (NVIDIA GeForce RTX 2080 Ti):
  总显存: 10.75 GiB
  已分配显存: 0.71 GiB
  预留缓存显存: 0.78 GiB
  可用显存: 10.05 GiB
  峰值已分配显存: 0.71 GiB
GPU 1 (NVIDIA GeForce RTX 2080 Ti):
  总显存: 10.75 GiB
  已分配显存: 0.00 GiB
  预留缓存显存: 0.00 GiB
  可用显存: 10.75 GiB
  峰值已分配显存: 0.00 GiB
GPU 2 (NVIDIA GeForce RTX 2080 Ti):
  总显存: 10.75 GiB
  已分配显存: 0.00 GiB
  预留缓存显存: 0.00 GiB
  可用显存: 10.75 GiB
  峰值已分配显存: 0.00 GiB
GPU 3 (NVIDIA GeForce RTX 2080 Ti):
  总显存: 10.75 GiB
  已分配显存: 0.00 GiB
  预留缓存显存: 0.00 GiB
  可用显存: 10.75 GiB
  峰值已分配显存: 0.00 GiB
GPU 4 (NVIDIA GeForce RTX 2080 Ti):
  总显存: 10.75 GiB
  已分配显存: 0.00 GiB
  预留缓存显存: 0.00 GiB
  可用显存: 10.75 GiB
  峰值已分配显存: 0.00 GiB
GPU 5 (NVIDIA GeForce RTX 2080 Ti):
  总显存: 10.75 GiB
  已分配显存: 0.00 GiB
  预留缓存显存: 0.00 GiB
  可用显存: 10.75 GiB
  峰值已分配显存: 0.00 GiB
GPU 6 (NVIDIA GeForce RTX 2080 Ti):
  总显存: 10.75 GiB
  已分配显存: 0.00 GiB
  预留缓存显存: 0.00 GiB
  可用显存: 10.75 GiB
  峰值已分配显存: 0.00 GiB
GPU 7 (NVIDIA GeForce RTX 2080 Ti):
  总显存: 10.75 GiB
  已分配显存: 0.00 GiB
  预留缓存显存: 0.00 GiB
  可用显存: 10.75 GiB
  峰值已分配显存: 0.00 GiB
Rank 0 Epoch 1/2:   0%|                                                                                                                | 0/93 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/data/bdd100k/sd/DiT_from_scratch/stable_diffusion_from_scratch/train_stable_diffusion_DDP.py", line 331, in <module>
    main()
  File "/data/bdd100k/sd/DiT_from_scratch/stable_diffusion_from_scratch/train_stable_diffusion_DDP.py", line 216, in main
    for batch in train_dataloader:
  File "/data/miniconda3/envs/pose_det/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 652, in __next__
    data = self._next_data()
  File "/data/miniconda3/envs/pose_det/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1347, in _next_data
    return self._process_data(data)
  File "/data/miniconda3/envs/pose_det/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1373, in _process_data
    data.reraise()
  File "/data/miniconda3/envs/pose_det/lib/python3.8/site-packages/torch/_utils.py", line 461, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/data/miniconda3/envs/pose_det/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 302, in _worker_loop
    data = fetcher.fetch(index)
  File "/data/miniconda3/envs/pose_det/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 49, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/data/miniconda3/envs/pose_det/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 49, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/data/bdd100k/sd/DiT_from_scratch/stable_diffusion_from_scratch/train_stable_diffusion_DDP.py", line 81, in __getitem__
    latent = self.model.module.encode(augmented_image.unsqueeze(0).to(self.device))
  File "/data/miniconda3/envs/pose_det/lib/python3.8/site-packages/torch/cuda/__init__.py", line 207, in _lazy_init
    raise RuntimeError(
RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method
